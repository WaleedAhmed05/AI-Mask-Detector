{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76980745",
   "metadata": {},
   "source": [
    "# Creating a csv file that stores filenames, classes and source for mask classification network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a235fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating categories for dataset\n",
    "import os, csv\n",
    "\n",
    "base_path = os.getcwd()\n",
    "mask_categories = [\"cloth_mask\", \"N-95_Mask\", \"no_mask\", \"surgical_mask\", \"wrong_worn_mask\"]\n",
    "# categories_size = [0, 0, 0, 0, 0]\n",
    "dataset_csv = os.path.join(base_path, \"dataset.csv\")\n",
    "with open(dataset_csv, 'w+') as file:\n",
    "    writer=csv.writer(file)\n",
    "    writer.writerow([\"\", \"filename\", \"Category\"])\n",
    "    for dataset_path, dataset_dirs, dataset_files in os.walk(os.path.join(base_path, \"dataset\")):\n",
    "        for category_index, mask_category in enumerate(mask_categories):\n",
    "            counter = 0\n",
    "            for category_path, category_dirs, category_files in os.walk(os.path.join(dataset_path, mask_category)):\n",
    "                for file in category_files:\n",
    "                    writer.writerow([counter, os.path.join(category_path, file), category_index])\n",
    "                    counter += 1\n",
    "\n",
    "# csv file has the following columns:\n",
    "# 0: index\n",
    "# 1: filename\n",
    "# 2: category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e599c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6064b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b033d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torch) (0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b9d67c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (0.11.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: torch==1.10.2 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torchvision) (1.10.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torch==1.10.2->torchvision) (4.1.1)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\waleed\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torch==1.10.2->torchvision) (0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb24a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7ea769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(base_path, \"dataset.csv\")\n",
    "img_path = os.path.join(base_path, \"dataset\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1f62710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cloth_path=\"C:/Users/Waleed/Downloads/Code/dataset/cloth_mask\"\n",
    "cloth_mask_len= len(os.listdir(cloth_path))\n",
    "\n",
    "N_95_Mask_path=\"C:/Users/Waleed/Downloads/Code/dataset/N-95_Mask\"\n",
    "N_95_Mask_len= len(os.listdir(N_95_Mask_path))\n",
    "\n",
    "no_mask_path=\"C:/Users/Waleed/Downloads/Code/dataset/no_mask\"\n",
    "no_mask_len= len(os.listdir(no_mask_path))\n",
    "\n",
    "surgical_mask_path=\"C:/Users/Waleed/Downloads/Code/dataset/surgical_mask\"\n",
    "surgical_mask_len= len(os.listdir(surgical_mask_path))\n",
    "\n",
    "wrong_worn_mask_path=\"C:/Users/Waleed/Downloads/Code/dataset/wrong_worn_mask\"\n",
    "wrong_worn_mask_len= len(os.listdir(wrong_worn_mask_path))\n",
    "\n",
    "print(cloth_mask_len)\n",
    "print(N_95_Mask_len)\n",
    "print(no_mask_len)\n",
    "print(surgical_mask_len)\n",
    "print(wrong_worn_mask_len)\n",
    "\n",
    "\n",
    "plt.title('Distribution of classes ')\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
    "plt.bar(\"cloth_mask\", cloth_mask_len)\n",
    "plt.bar(\"N-95_Mask\", N_95_Mask_len)\n",
    "plt.bar(\"no_mask\", no_mask_len)\n",
    "plt.bar(\"surgical_mask\", surgical_mask_len)\n",
    "plt.bar(\"Wrong worn\", wrong_worn_mask_len)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624847ba",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37fc052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class MaskImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, index): # index is the row number\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[index, 1])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.img_labels.iloc[index, 2]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7fcddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "#Dimension of each image -> 224* 224\n",
    "transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                transforms.ToTensor(), # tensor means convert to numpy array\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                ]) # 0.485, 0.456, 0.406 is the mean of ImageNet dataset\n",
    "                                # 0.229, 0.224, 0.225 is the std of ImageNet dataset\n",
    "\n",
    "full_dataset = MaskImageDataset(data_path, img_path, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7d9dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.75 * len(full_dataset)) # 75% of the dataset is used for training\n",
    "test_size = len(full_dataset) - train_size # 25% of the dataset is used for testing\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abd2a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "train_data = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_data = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d7251",
   "metadata": {},
   "source": [
    "# Creating Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a60df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d, Linear, MaxPool2d, Module, BatchNorm2d\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MaskNetV2(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        ''' Initializing the model'''\n",
    "        super(MaskNetV2, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=2)\n",
    "        self.bn_1 = BatchNorm2d(32) # batch normalization makes the network more robust to variations in the input data\n",
    "        \n",
    "        self.conv2 = Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=2) \n",
    "        self.bn_2 = BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=2)\n",
    "        self.bn_3 = BatchNorm2d(128)\n",
    "        \n",
    "#         self.conv4 = Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=2)\n",
    "#         self.bn_4 = BatchNorm2d(256)\n",
    "        \n",
    "        self.maxpool = MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.fc1 = Linear(2048, 256)\n",
    "        self.fc2 = Linear(256, 64)\n",
    "        self.fc3 = Linear(64, 5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.bn_1(self.conv1(x)), inplace=True) # relu is good for non-linear activation\n",
    "        x = self.maxpool(x) # maxpool is good for downsampling\n",
    "        \n",
    "        x = F.relu(self.bn_2(self.conv2(x)), inplace=True)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = F.relu(self.bn_3(self.conv3(x)), inplace=True)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "#         x = F.relu(self.bn_4(self.conv4(x)), inplace=True)\n",
    "#         x = self.maxpool(x)\n",
    "        \n",
    "        # Flattening layer followed by the convolutional neural network\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = F.dropout(x, 0.3, training=self.training) # dropout is good for regularization\n",
    "        \n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "        x = F.dropout(x, 0.3, training=self.training)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1764d6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskNetV2(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      "  (bn_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      "  (bn_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      "  (bn_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MaskNetV2()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6289fc3",
   "metadata": {},
   "source": [
    "Creating two variants of the above model - one without any padding and one without any pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariantWithoutPadding(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        ''' Initializing the model'''\n",
    "        super(VariantWithoutPadding, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=0)\n",
    "        self.bn_1 = BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=0)\n",
    "        self.bn_2 = BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=0)\n",
    "        self.bn_3 = BatchNorm2d(128)\n",
    "        \n",
    "#         self.conv4 = Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=0)\n",
    "#         self.bn_4 = BatchNorm2d(256)\n",
    "        \n",
    "        self.maxpool = MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.fc1 = Linear(1152, 256) \n",
    "        self.fc2 = Linear(256, 64)\n",
    "        self.fc3 = Linear(64, 5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.bn_1(self.conv1(x)), inplace=True) # relu is used to prevent the negative values\n",
    "        x = self.maxpool(x) # maxpool is used to reduce the size of the input for the next layer\n",
    "        \n",
    "        x = F.relu(self.bn_2(self.conv2(x)), inplace=True)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = F.relu(self.bn_3(self.conv3(x)), inplace=True)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "#         x = F.relu(self.bn_4(self.conv4(x)), inplace=True)\n",
    "#         x = self.maxpool(x)\n",
    "        \n",
    "        # Flattening layer followed by the convulation network\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = F.dropout(x, 0.3, training=self.training) # dropout is used to prevent overfitting\n",
    "        \n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "        x = F.dropout(x, 0.3, training=self.training) # dropout is used to prevent overfitting\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_without_padding = VariantWithoutPadding()\n",
    "print(variant_without_padding)\n",
    "\n",
    "variant_without_padding = variant_without_padding.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0902b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariantWithoutPooling(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        ''' Initializing the model'''\n",
    "        super(VariantWithoutPooling, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=2)\n",
    "        self.bn_1 = BatchNorm2d(32) # batch normalization is used to prevent the network from overfitting\n",
    "        \n",
    "        self.conv2 = Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=2) \n",
    "        self.bn_2 = BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=2)\n",
    "        self.bn_3 = BatchNorm2d(128)\n",
    "        \n",
    "#         self.conv4 = Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=2)\n",
    "#         self.bn_4 = BatchNorm2d(256)\n",
    "        \n",
    "        self.fc1 = Linear(115200, 4096) # 115200 is the number of features after the convolutional neural network\n",
    "        self.fc2 = Linear(4096, 512)\n",
    "        self.fc3 = Linear(512, 5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.bn_1(self.conv1(x)), inplace=True) # relu is used to prevent the negative values\n",
    "        \n",
    "        x = F.relu(self.bn_2(self.conv2(x)), inplace=True)\n",
    "        \n",
    "        x = F.relu(self.bn_3(self.conv3(x)), inplace=True)\n",
    "        \n",
    "#         x = F.relu(self.bn_4(self.conv4(x)), inplace=True)\n",
    "        \n",
    "        # Flattening layer followed by the fourth convulation network\n",
    "        x = x.view(x.size(0), -1) \n",
    "        \n",
    "        x = F.relu(self.fc1(x), inplace=True)  \n",
    "        x = F.dropout(x, 0.3, training=self.training) # dropout is used to prevent overfitting\n",
    "        \n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "        x = F.dropout(x, 0.3, training=self.training)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_without_pooling = VariantWithoutPooling()\n",
    "print(variant_without_pooling)\n",
    "\n",
    "variant_without_pooling = variant_without_pooling.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96844e",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device) # model is moved to the GPU if available\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def trainModel(model, model_name, train_loader, device):\n",
    "    learning_rate = 0.001\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(),lr = learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    acc_list = []\n",
    "    epochs = 45\n",
    "    \n",
    "    for i in range(1, epochs+1): \n",
    "        start = time.time()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        for j, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # optimizer is set to zero before the forward pass\n",
    "            outputs = model(inputs) # forward pass\n",
    "            \n",
    "            loss = criterion(outputs, labels) # loss is calculated\n",
    "            loss.backward() # loss is backpropagated\n",
    "            optimizer.step() # optimizer is updated\n",
    "            \n",
    "            running_loss += loss.item() # loss is added to the running loss\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1) # predicted is the index of the maximum value in the output\n",
    "            correct += (predicted == labels).sum().item()  # correct is the number of correct predictions\n",
    "        \n",
    "        train_loss = running_loss/len(train_loader.sampler) # loss is divided by the number of samples in the training set\n",
    "        train_losses.append(train_loss) \n",
    "        accuracy = (correct / total) * 100 # accuracy is calculated\n",
    "        acc_list.append(accuracy)\n",
    "        \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tAccuracy: {:.6f}'.format(\n",
    "        i, train_loss, accuracy))\n",
    "        elapsed = time.time() - start\n",
    "        print(\"Elapsed time: \" + time.strftime(\"%H:%M:%S.{}\".format(str(elapsed % 1)[2:])[:11], time.gmtime(elapsed)))\n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "    torch.save(model.state_dict(), os.path.join(base_path, \"saved_models\\\\\" + model_name + \".pt\"))\n",
    "    torch.save(model, os.path.join(base_path, \"saved_models\\\\\" + model_name + \"full.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c8953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainModel(model, train_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18409477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variant_without_padding = VariantWithoutPadding()\n",
    "# variant_without_pooling = VariantWithoutPooling()\n",
    "# print(variant_without_padding)\n",
    "# print(variant_without_pooling)\n",
    "\n",
    "# variant_without_padding = variant_without_padding.to(device)\n",
    "# variant_without_pooling = variant_without_pooling.to(device)\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510c950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainModel(model, \"CNN Model with padding and pooling\", train_data, device) \n",
    "trainModel(variant_without_padding, \"CNN Model without padding\", train_data, device)\n",
    "trainModel(variant_without_pooling, \"CNN Model without pooling\", train_data, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e319a",
   "metadata": {},
   "source": [
    "# Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de885dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import  DataLoader,random_split\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset.csv\")\n",
    "img_path = os.path.join(base_path, \"dataset\", \"\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "#device = torch.device('cpu')\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "full_dataset = MaskImageDataset(data_path, img_path, transform=transform) \n",
    "\n",
    "train_size = int(0.75 * len(full_dataset)) \n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "#train_data = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_data = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "the_model = MaskNetV2() \n",
    "the_model = the_model.to(device)\n",
    "path = os.path.join(base_path, \"saved_models\\\\CNN Model with padding and poolingfull.pt\")\n",
    "the_model=torch.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b29ff",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7adc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "def test_model(model,testing_data,DEVICE):\n",
    "      \n",
    "    testing_loss = 0\n",
    "    correct_prediction = 0 \n",
    "    data_size = 0\n",
    "    prediction1=[]\n",
    "    for images, labels in testing_data:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)          \n",
    "            data_size += len(images)\n",
    "            prediction = model(images) \n",
    "            \n",
    "            prediction1.append(prediction)\n",
    "            \n",
    "            testing_loss += cross_entropy(prediction, labels).item()\n",
    "            correct_prediction += (prediction.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "\n",
    "    accuracy = correct_prediction/data_size\n",
    "    testing_loss = testing_loss/data_size\n",
    "\n",
    "    print('\\nTesting:')\n",
    "    print(f\"Correct prediction: {correct_prediction}/{data_size} and accuracy: {accuracy} and loss: {testing_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1665b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "test_model(the_model,test_data,device)\n",
    "torch.cuda.empty_cache()\n",
    "test_model(variant_without_padding,test_data,device)\n",
    "torch.cuda.empty_cache()\n",
    "test_model(variant_without_pooling,test_data,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00947a1",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1895b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "def get_labels_N_prediction(model,loader,DEVICE):\n",
    "    all_labels = []\n",
    "    all_prediction = []\n",
    "\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        images = images.to(DEVICE)\n",
    "\n",
    "        prediction = model(images).to(torch.device(\"cpu\")).argmax(dim=1).detach().numpy() \n",
    "        labels = labels.to(torch.device(\"cpu\")).detach().numpy() \n",
    "\n",
    "        all_prediction = np.append(all_prediction,prediction)\n",
    "        all_labels = np.append(all_labels,labels)\n",
    "\n",
    "    return [all_labels,all_prediction]\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    labels_N_prediction = get_labels_N_prediction(the_model, test_data, device)\n",
    "    \n",
    "print(classification_report(labels_N_prediction[0], labels_N_prediction[1]))\n",
    "conf_matrix = confusion_matrix(labels_N_prediction[0], labels_N_prediction[1]) \n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"Confusion matrices for variant without padding->\")\n",
    "with torch.no_grad():\n",
    "    labels_N_prediction_variant_1 = get_labels_N_prediction(variant_without_padding, test_data, device)\n",
    "    \n",
    "print(classification_report(labels_N_prediction_variant_1[0], labels_N_prediction_variant_1[1]))\n",
    "conf_matrix = confusion_matrix(labels_N_prediction_variant_1[0], labels_N_prediction_variant_1[1])\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"Confusion matrices for variant without pooling->\")\n",
    "with torch.no_grad():\n",
    "    labels_N_prediction_variant_2 = get_labels_N_prediction(variant_without_pooling, test_data, device)\n",
    "    \n",
    "print(classification_report(labels_N_prediction[0], labels_N_prediction[1]))\n",
    "conf_matrix = confusion_matrix(labels_N_prediction_variant_2[0], labels_N_prediction_variant_2[1])\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e79c792",
   "metadata": {},
   "source": [
    "## Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb090b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import pandas as pd\n",
    "#ACCURACY SCORE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib\n",
    "#y_pred = the_model.predict(test_dataset)\n",
    "y_pred =labels_N_prediction[1]  \n",
    "y_test =labels_N_prediction[0] \n",
    "print(\"Accuracy of CNN Model with padding and pooling : \",accuracy_score(y_test, y_pred))\n",
    "\n",
    "y_pred_variant_1 =labels_N_prediction_variant_1[1]\n",
    "y_test_variant_1 =labels_N_prediction_variant_1[0]\n",
    "print(\"Accuracy of CNN Model variant without padding : \",accuracy_score(y_test_variant_1, y_pred_variant_1))\n",
    "\n",
    "y_pred_variant_2 =labels_N_prediction_variant_2[1]\n",
    "y_test_variant_2 =labels_N_prediction_variant_2[0]\n",
    "print(\"Accuracy of CNN Model variant without pooling : \",accuracy_score(y_test_variant_2, y_pred_variant_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4066b81",
   "metadata": {},
   "source": [
    "## Precision, Recal, FScore, Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRECISION , RECALL,FSCORE,SUPPORT\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "d=()\n",
    "d=precision_recall_fscore_support(labels_N_prediction[0], y_pred)\n",
    "prec = d[0].tolist()\n",
    "recall = d[1].tolist()\n",
    "fscore = d[2].tolist()\n",
    "support = d[3].tolist()\n",
    "#precision_recall_fscore_support(y_test, y_pred,average=\"macro\")\n",
    "print(\"prec --\",prec)\n",
    "print(\"recall --\",recall)\n",
    "print(\"fscore --\",fscore)\n",
    "print(\"support --\",support)\n",
    "\n",
    "print(\"for variant 1 ->\")\n",
    "d_variant_1=()\n",
    "d_variant_1=precision_recall_fscore_support(labels_N_prediction_variant_1[0], y_pred_variant_1)\n",
    "prec_variant_1 = d_variant_1[0].tolist()\n",
    "recall_variant_1 = d_variant_1[1].tolist()\n",
    "fscore_variant_1 = d_variant_1[2].tolist()\n",
    "support_variant_1 = d_variant_1[3].tolist()\n",
    "print(\"prec --\",prec_variant_1)\n",
    "print(\"recall --\",recall_variant_1)\n",
    "print(\"fscore --\",fscore_variant_1)\n",
    "print(\"support --\",support_variant_1)\n",
    "\n",
    "print(\"for variant 2 ->\")\n",
    "d_variant_2=()\n",
    "d_variant_2=precision_recall_fscore_support(labels_N_prediction_variant_2[0], y_pred_variant_2)\n",
    "prec_variant_2 = d_variant_2[0].tolist()\n",
    "recall_variant_2 = d_variant_2[1].tolist()\n",
    "fscore_variant_2 = d_variant_2[2].tolist()\n",
    "support_variant_2 = d_variant_2[3].tolist()\n",
    "print(\"prec --\",prec_variant_2)\n",
    "print(\"recall --\",recall_variant_2)\n",
    "print(\"fscore --\",fscore_variant_2)\n",
    "print(\"support --\",support_variant_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ad832",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e128353",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae9ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CLASSIFICATION REPORT - PRECISION , RECALL,FSCORE,SUPPORT\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#PLOTTING CLASSIFICATION REPORT\n",
    "import seaborn as sns\n",
    "from pylab import savefig\n",
    "h= classification_report(y_test, y_pred , output_dict=True)\n",
    "svm =sns.heatmap(pd.DataFrame(h).iloc[:-1, :].T, annot=True)\n",
    "figure = svm.get_figure()\n",
    "\n",
    "figure.savefig('Classification report.png')\n",
    "\n",
    "#CONFUSION MATRIX & PLOTTING IT\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred).tolist()\n",
    "cm = sns.heatmap(confusion_matrix, annot=True, fmt='d')\n",
    "\n",
    "\n",
    "cm.plot()\n",
    "matplotlib.pyplot.show()\n",
    "matplotlib.pyplot.savefig('confusion metrics.png')\n",
    "\n",
    "#PLOTTING PRECISION AND RECALL GRAPH\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "disp = PrecisionRecallDisplay(precision=precision_recall_fscore_support(y_test, y_pred)[0],recall=precision_recall_fscore_support(y_test, y_pred)[1])\n",
    "disp.plot()\n",
    "matplotlib.pyplot.show()\n",
    "matplotlib.pyplot.savefig('Precision vs Recall.png')\n",
    "\n",
    "\n",
    "# For Variant 1\n",
    "#PLOTTING CLASSIFICATION REPORT\n",
    "h_variant_1= classification_report(y_test, y_pred_variant_1 , output_dict=True)\n",
    "svm_variant_1 =sns.heatmap(pd.DataFrame(h_variant_1).iloc[:-1, :].T, annot=True)\n",
    "figure_variant_1 = svm_variant_1.get_figure()\n",
    "figure_variant_1.savefig('Classification report_variant_1.png')\n",
    "\n",
    "#CONFUSION MATRIX & PLOTTING IT\n",
    "# confusion_matrix_variant_1 = confusion_matrix(y_test, y_pred_variant_1).tolist()\n",
    "# cm_variant_1 = sns.heatmap(confusion_matrix_variant_1, annot=True, fmt='d')\n",
    "\n",
    "\n",
    "# cm_variant_1.plot()\n",
    "# matplotlib.pyplot.show()\n",
    "# matplotlib.pyplot.savefig('confusion metrics_variant_1.png')\n",
    "\n",
    "# #PLOTTING PRECISION AND RECALL GRAPH\n",
    "# disp_variant_1 = PrecisionRecallDisplay(precision=precision_recall_fscore_support(y_test_variant_1, y_pred_variant_1)[0],recall=precision_recall_fscore_support(y_test_variant_1, y_pred_variant_1)[1])\n",
    "# disp_variant_1.plot()\n",
    "# matplotlib.pyplot.show()\n",
    "# matplotlib.pyplot.savefig('Precision vs Recall_variant_1.png')\n",
    "\n",
    "\n",
    "\n",
    "# For Variant 2\n",
    "#PLOTTING CLASSIFICATION REPORT\n",
    "h_variant_2= classification_report(y_test, y_pred_variant_2 , output_dict=True)\n",
    "svm_variant_2 =sns.heatmap(pd.DataFrame(h_variant_2).iloc[:-1, :].T, annot=True)\n",
    "figure_variant_2 = svm_variant_2.get_figure()\n",
    "figure_variant_2.savefig('Classification report_variant_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec335147",
   "metadata": {},
   "source": [
    "# Testing trained model on new Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5ef26",
   "metadata": {},
   "source": [
    "## Loading saved trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(os.path.join(base_path, \"saved_models\\\\CNN Model with padding and poolingfull.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a4044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predictImage(model, imagePath, device, labels={0:'cloth', 1:'N95', 2:'No Mask', 3:'Surgical', 4:'Worn Wrong'}):\n",
    "    \n",
    "    \n",
    "    transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "    image = Image.open(imagePath).convert('RGB')\n",
    "    imageD = Image.open(imagePath).convert('RGB')\n",
    "    \n",
    "    image = transform(image)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        output = model(image.unsqueeze(0))\n",
    "    pred = list(output.argmax(dim=1).cpu().numpy())\n",
    "    #print(pred)\n",
    "    \n",
    "    plt.imshow(imageD)    \n",
    "    print(\"Prediction : \" + labels[pred[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictImage(model, os.path.join(base_path, \"try1.jpg\"), \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f0579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad21a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62978be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546381f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d5e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302aa94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
